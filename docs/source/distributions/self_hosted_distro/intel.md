<!-- This file was auto-generated by distro_codegen.py, please edit source -->
# INTEL Distribution

The `llamastack/distribution-intel` distribution consists of the following provider configurations.

| API | Provider(s) |
|-----|-------------|
| agents | `inline::meta-reference` |
| datasetio | `inline::localfs` |
| eval | `inline::meta-reference` |
| inference | `remote::intel` |
| safety | `inline::llama-guard` |
| scoring | `inline::basic` |
| telemetry | `inline::meta-reference` |
| tool_runtime | `inline::rag-runtime` |
| vector_io | `inline::faiss` |


### Environment Variables

The following environment variables can be configured:

- `INTEL_API_KEY`: INTEL API Key (default: ``)
- `INTEL_BASE_URL`: Base URL for Intel Inference Endpoint (default: `https://inference.api.intel.com`)
- `INFERENCE_MODEL`: Inference model (default: `Llama3.3-70B-Instruct`)
- `SAFETY_MODEL`: Name of the model to use for safety (default: `meta/llama-3.1-8b-instruct`)

### Models

The following models are available by default:

- `meta-llama/Llama-3.3-70B-Instruct (aliases: meta-llama/Llama-3.3-70B-Instruct)`
- `meta-llama/Llama-3.2-1B-Instruct (aliases: meta-llama/Llama-3.2-1B-Instruct)`
- `meta-llama/Llama-3.2-3B-Instruct (aliases: meta-llama/Llama-3.2-3B-Instruct)`
- `meta-llama/Llama-3.1-70B-Instruct (aliases: meta-llama/Llama-3.1-70B-Instruct)`
- `meta-llama/Meta-Llama-3.1-8B-Instruct (aliases: meta-llama/Llama-3.1-8B-Instruct)`


## Prerequisites
### INTEL API Keys

Make sure you have access to a INTEL API Key. Use this key for the `INTEL_API_KEY` environment variable.


## Running Llama Stack with INTEL

You can do this via Conda or venv (build code), or Docker which has a pre-built image.

### Via Docker

This method allows you to get started quickly without having to build the distribution code.

```bash
LLAMA_STACK_PORT=8321
docker run \
  -it \
  --pull always \
  -p $LLAMA_STACK_PORT:$LLAMA_STACK_PORT \
  -v ./run.yaml:/root/my-run.yaml \
  llamastack/distribution-intel \
  --config /root/my-run.yaml \
  --port $LLAMA_STACK_PORT \
  --env INTEL_API_KEY=$INTEL_API_KEY
```

### Via Conda

```bash
INFERENCE_MODEL=meta-llama/Llama-3.1-8b-Instruct
llama stack build --template intel --image-type conda
llama stack run ./run.yaml \
  --port 8321 \
  --env INTEL_API_KEY=$INTEL_API_KEY \
  --env INFERENCE_MODEL=$INFERENCE_MODEL
```

### Via venv

If you've set up your local development environment, you can also build the image using your local virtual environment.

```bash
INFERENCE_MODEL=meta-llama/Llama-3.1-8b-Instruct
llama stack build --template intel --image-type venv
llama stack run ./run.yaml \
  --port 8321 \
  --env INTEL_API_KEY=$INTEL_API_KEY \
  --env INFERENCE_MODEL=$INFERENCE_MODEL
```
